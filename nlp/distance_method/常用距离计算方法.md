## 常用距离计算方法

[TOC]



### 一、欧氏距离(Euclidean Distance)

#### 1.1 定义

> 欧氏距离是最常用的距离计算公式，**衡量的是多维空间中各个点之间的绝对距离**，当数据很稠密并且连续时，这是一种很好的计算方式。

#### 1.2 公式

$$
dist(X, Y) = \sqrt{\sum_{i=1}^{n}(x_i - yi)^2}
$$

#### 1.3 特点

> （1）因为计算是基于各维度特征的绝对数值，所以欧氏度量需要保证各维度指标在相同的刻度级别
>
> （2）比如：对身高（cm）和体重（kg）两个单位不同的指标使用欧式距离可能使结果失效。

#### 1.4 适用数据格式

> （1）数据很稠密并且连续时，这是一种很好的计算方式。
>
> （2）因为计算是基于各维度特征的绝对数值，所以欧氏度量需要保证各维度指标在相同的刻度级别，如在KNN中需要对特征进行归一化。
>
> （3）欧氏距离能够体现个体数值特征的绝对差异，所以更多的用于需要从维度的数值大小中体现差异的分析，如使用用户行为指标分析用户价值的相似度或差异。

#### 1.5 Python实现

```python
import numpy as np
 
x=np.random.random(10)
y=np.random.random(10)
 
# solution1
dist1 = np.linalg.norm( x - y )
 
# solution2
dist2 = np.sqrt(np.sum(np.square(x - y)))  
 
print('x',x)
print('y',y)
print('dist1',dist1)
print('dist2',dist2)
```

### 二、余弦距离

#### 1.1 定义

> 余弦相似度用**向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小**。相比距离度量，余弦相似度更加注重两个向量在方向上的差异，而非距离或长度上。

#### 1.2 公式

![image-20200825011746073](/Users/ssh/Documents/private/project/github/programmer-learning-notes/nlp/distance_method/常用距离计算方法.assets/image-20200825011746073.png)

#### 1.3 特点

> 相比距离度量，余弦相似度更加注重两个向量在方向上的差异，而非距离或长度上。

#### 1.4 适用的数据格式

> 适合word2vec模型向量化的数据
>
> 余弦距离更多的是从方向上区分差异，而对绝对的数值不敏感，更多的用于使用用户对内容评分来区分兴趣的相似度和差异，同时修正了用户间可能存在的度量标准不统一的问题（因为余弦距离对绝对数值不敏感）。

#### 1.5 Python实现

```python
import numpy as np
from scipy.spatial.distance import pdist
 
x=np.random.random(10)
y=np.random.random(10)
 
# solution1
dist1 = 1 - np.dot(x,y)/(np.linalg.norm(x)*np.linalg.norm(y))
 
# solution2
dist2 = pdist(np.vstack([x,y]),'cosine')
 
print('x',x)
print('y',y)
print('dist1',dist1)
print('dist2',dist2)
```

#### 1.6 与欧式距离的区别

> 余弦距离使用两个向量夹角的余弦值作为衡量两个个体间差异的大小。相比欧氏距离，余弦距离更加注重两个向量在方向上的差异。

![clip_image009](http://images.cnitblog.com/blog/407700/201306/28144620-099ab1aaff8a4d139a2157f1eaf13d0b.jpg)



> 从上图可以看出：
>
> ​	（1）欧式距离衡量的是空间各点的绝对距离，跟各个点所在的位置坐标直接相关；
>
> ​	（2）余弦距离衡量的是空间向量的夹角，更加体现在方向上的差异，而不是位置。

### 三、曼哈顿距离

#### 1.1 定义

> 曼哈顿距离简单来说就是统计相同坐标轴上的距离的和。

#### 1.2 公式

![image-20200825013300892](/Users/ssh/Documents/private/project/github/programmer-learning-notes/nlp/distance_method/常用距离计算方法.assets/image-20200825013300892.png)

#### 1.3 Python实现

```python
import numpy as np

x=np.random.random(10)
y=np.random.random(10)

#方法一：根据公式求解
d1=np.sum(np.abs(x-y))

#方法二：根据scipy库求解
from scipy.spatial.distance import pdist
X=np.vstack([x,y])
d2=pdist(X,'cityblock')
```



### 四、闵可夫斯基距离(Minkowski Distance)

#### 1.1 定义

> 欧式距离和曼哈顿距离在形式上比较相近，其实，它们是闵可夫斯基距离的特殊化。

#### 1.2 公式

![image-20200825013434388](/Users/ssh/Documents/private/project/github/programmer-learning-notes/nlp/distance_method/常用距离计算方法.assets/image-20200825013434388.png)

#### 1.3 适用数据格式

> 适合TF-IDF向量化后的数据或者提炼出来的主题模型数据

#### 1.4 Python实现

```python
import numpy as np

x=np.random.random(10)
y=np.random.random(10)

#方法一：根据公式求解,p=2
d1=np.sqrt(np.sum(np.square(x-y)))

#方法二：根据scipy库求解
from scipy.spatial.distance import pdist
X=np.vstack([x,y])
d2=pdist(X,'minkowski',p=2)
```



### 五、皮尔森相关系数(pearson)

#### 1.1 定义

> 皮尔森相关系数是衡量线性关联性的程度。

#### 1.2 公式

![image-20200825013613911](/Users/ssh/Documents/private/project/github/programmer-learning-notes/nlp/distance_method/常用距离计算方法.assets/image-20200825013613911.png)

> 两个连续变量(𝑋,𝑌)(X,Y)的pearson相关性系数𝑃x,y等于它们之间的协方差𝑐𝑜𝑣(𝑋,𝑌)除以它们各自标准差的乘积𝜎X,𝜎𝑌。系数的取值总是在-1.0到1.0之间，接近0的变量被成为无相关性，接近1或者-1被称为具有强相关性。

### 六、Jaccard相似性系数

#### 1.1 定义

> Jaccard（杰卡德）相似性系数主要用于计算符号度量或布尔值度量的样本间的相似度。

#### 1.2 公式

![image-20200825013848570](/Users/ssh/Documents/private/project/github/programmer-learning-notes/nlp/distance_method/常用距离计算方法.assets/image-20200825013848570.png)

#### 1.3 适用数据格式

> 若样本间的特征属性由符号和布尔值标识，无法衡量差异具体值的大小，只能获得“是否相同”这样一种结果，而Jaccard系数关心的是样本间共同具有的特征。适合词集模型向量化的数据。



------

> [【Python】欧氏距离和余弦距离](https://blog.csdn.net/mr_evanchen/article/details/77511312)
>
> [机器学习基础：相似度和距离度量究竟是什么](https://baijiahao.baidu.com/s?id=1646892223702543354&wfr=spider&for=pc)
>
> [文本相似度计算-距离的度量](https://www.cnblogs.com/huangyc/p/9786731.html)
>
> [距离度量以及python实现(一)](https://www.cnblogs.com/wyuzl/p/7867979.html)

